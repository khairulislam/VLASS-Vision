{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1add808",
   "metadata": {},
   "source": [
    "# AstroCLIP\n",
    "\n",
    "This notebook finetunes the astroclip model on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8369500f",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dad0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mi3se\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:209: Attribute 'image_encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['image_encoder'])`.\n",
      "c:\\Users\\mi3se\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:209: Attribute 'spectrum_encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['spectrum_encoder'])`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CLIPLoss loss\n"
     ]
    }
   ],
   "source": [
    "from astroclip.models import AstroClipModel\n",
    "\n",
    "model = AstroClipModel.load_from_checkpoint(\n",
    "    \"../../OSS/AstroCLIP/pretrained/astroclip.ckpt\"\n",
    ").cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef2f3fe",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0f9d253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((144, 144)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5cd2803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datamodule import VLASSLoader\n",
    "datamodule = VLASSLoader(\n",
    "    root = \"./data\", batch_size=32, pin_memory=True,\n",
    "    transform = transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec60090",
   "metadata": {},
   "source": [
    "## LinearProbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8104fba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from models.vision import VisionModel\n",
    "\n",
    "class AstroClip(VisionModel):\n",
    "    def __init__(self, num_classes=4, embed_dim=1024, freeze=True):\n",
    "        super().__init__(num_classes=num_classes)\n",
    "        self.backbone = model\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "        if freeze:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x, input_type='image')\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "516e5eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model = AstroClip().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1174aa76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file: ./data\\vlass_data_array.p\n",
      "Loading labels from file: ./data\\vlass_labels.p\n",
      "61351 images, each of size 64 x 64 pixels.\n",
      "There are 61351 corresponding labels - one category for each image.\n",
      "Loading data from file: ./data\\vlass_data_array.p\n",
      "Loading labels from file: ./data\\vlass_labels.p\n",
      "61351 images, each of size 64 x 64 pixels.\n",
      "There are 61351 corresponding labels - one category for each image.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type           | Params | Mode \n",
      "----------------------------------------------------\n",
      "0 | backbone | AstroClipModel | 370 M  | eval \n",
      "1 | head     | Linear         | 4.1 K  | train\n",
      "----------------------------------------------------\n",
      "4.1 K     Trainable params\n",
      "370 M     Non-trainable params\n",
      "370 M     Total params\n",
      "1,483.210 Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "525       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070bebae44c24bd6bd04ed7cba49faf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mi3se\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val size: 12271\n",
      "Train size: 49080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mi3se\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd92ab521e454cb0ad51125cb4ae0f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=5` reached.\n"
     ]
    }
   ],
   "source": [
    "from lightning import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_steps=5, \n",
    "    accelerator='gpu', devices=1\n",
    ")\n",
    "trainer.fit(clip_model, datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e06496a",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167da947",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = datamodule.val_dataloader()\n",
    "clip_model = clip_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "11dba775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3, 4]\n",
    "a[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d070351c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 67/383 [01:38<07:46,  1.48s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m         predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     13\u001b[0m         y_trues\u001b[38;5;241m.\u001b[39mextend(labels\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m---> 14\u001b[0m         y_preds\u001b[38;5;241m.\u001b[39mextend(\u001b[43mpredicted\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     16\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy_score(y_trues, y_preds)\n\u001b[0;32m     17\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(y_trues, y_preds, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "clip_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_trues, y_preds = [], []\n",
    "    \n",
    "    for inputs, labels in tqdm(val_dataloader, total=len(val_dataloader)):\n",
    "        outputs = clip_model(inputs.cuda())\n",
    "        predicted = torch.argmax(outputs, 1)\n",
    "        \n",
    "        y_trues.extend(labels.detach().cpu().numpy())\n",
    "        y_preds.extend(predicted.detach().cpu().numpy())\n",
    "        \n",
    "acc = accuracy_score(y_trues, y_preds)\n",
    "f1 = f1_score(y_trues, y_preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2955bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import test_model\n",
    "result = test_model(clip_model, )\n",
    "print(f'Accuracy = {result[\"accuracy\"]}, F1 = {result[\"f1\"]}')\n",
    "print('Classification report:\\n', result['classification_report'])\n",
    "print('Confusion Matrix:\\n', result['confusion_matrix'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
